{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental goal\n",
    "1. Reduce the number of students left the free trial.\n",
    "2. Without significantly reducing the number of students completed the course.\n",
    "\n",
    "## Experiment Design\n",
    "### Metric Choice\n",
    "> List which metrics you will use as invariant metrics and evaluation metrics here.\n",
    "Explain both why you did or did not use it as an invariant metric and why you did or did not use it as an evaluation metric. Also, state what results you will look for in your evaluation metrics in order to launch the experiment.\n",
    "\n",
    "#### Invariant metrics\n",
    "* Number of cookies\n",
    "* Number of clicks \n",
    "* Click through probability\n",
    "\n",
    "**These are events happen before triggering the free trial screener so the numbers should be similar in both groups (experiment and control). We can know if the number of samples are equally assigned to both experiment and control groups from the value of \"Number of cookies\" and \"Number of clicks\". \"Click through probability\" is a ratio between 2 invarants so if the values are significantly different in control and experiment groups, we know something wrong with random assignment.**\n",
    "\n",
    "#### Evaluation metrics\n",
    "* Gross conversion: shows how many percentage of students deciding to enroll after they click \"Start free trial\". \n",
    "* Retention: shows the probability of payment, given enroll. \n",
    "* Net conversion: shows how many percentage of students deciding to pay after they click \"Start free trial\". \n",
    "\n",
    "**\"Enrollment\" and \"payment\" are events happens after the operation of the experiemnt (i.e., showing the screener). The number of enrollment directly reflects the effect of the experiment in reducing unready students, making gross conversion a good metric to evaluate our first goal. The number of payment direclt reflects how many motivated students want to complete the degree, making net conversion a good metric to evaluate our second goal. Retention, the ratio between payment and enrollment, captures the percentage of paid students in all enrolled students, making it a good metric for evaluating our second goal. Moreover, all the metrics are normalized so they are more robust to the differences in the sample sizes.  **\n",
    "\n",
    "#### Criteria for evaluation metrics for launching the experiment\n",
    "* **Gross conversion** The experiment will be launched only if a lower gross conversion in the experimental group is observed so we know the experiment indeed reduce students having less commitment in the enrollment with the newly added screener.\n",
    "\n",
    "* **Retention** The experiment will be launched only if the retention number for the experiment group is not significantly lower than the control group, so we can be certain that the screener don't have negative effect on the number of students proceed to the payment and finish the course. \n",
    "\n",
    "* **Net conversion** The experiment will be launched only if the net conversion in the experimental group is not significantly lower than the control group, so we can be certain that the screener won't significanlt reduce the number of students proceed to the payment and finish the course.\n",
    "\n",
    "\n",
    "#### Matrics didn't choose\n",
    "* **Number of user-ids** shows the number of enrolled users in the free trial, which gives similar information as gross conversion. However, the different sample sizes in experiment and control groups affects accuracy. Gross conversion, on the other hand, is normalized to cookies so it is more robust to the differences in the sample sizes. \n",
    "\n",
    "\n",
    "## Measuring Standard Deviation\n",
    ">List the standard deviation of each of your evaluation metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std of gross_conversion: 0.020230604137\n",
      "std of retention: 0.0549490121785\n",
      "std of net_conversion: 0.0156015445825\n"
     ]
    }
   ],
   "source": [
    "# gross_conversion\n",
    "gross = 0.20625\n",
    "std_gross = np.sqrt(gross*(1-gross)/3200)*np.sqrt(40000/5000)\n",
    "print 'std of gross conversion:', std_gross\n",
    "# Retention\n",
    "retention = 0.53\n",
    "std_retention = np.sqrt(retention*(1-retention)/660)*np.sqrt(40000/5000)\n",
    "print 'std of retention:', std_retention\n",
    "# net_conversion\n",
    "net = 0.1093125\n",
    "std_net = np.sqrt(net*(1-net)/3200)*np.sqrt(40000/5000)\n",
    "print 'std of net conversion:',std_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">For each of your evaluation metrics, indicate whether you think the analytic estimate would be comparable to the the empirical variability, or whether you expect them to be different (in which case it might be worth doing an empirical estimate if there is time). Briefly give your reasoning in each case.\n",
    "\n",
    "** Both \"gross conversion\" and \"net conversion\" use cookie as denominator. This suggests a comparable analytic estimate and empirical variability, as their unit of analysis matches their unit of diversion. On the other hand, the unit of analysis (i.e., number of enrollments) is different from the unit of diversion for Retention, implying a higher standard error using analytical estimation. Therefore, I will do the empirical estimate of Retention if I have time.**\n",
    "\n",
    "## Sizing\n",
    "\n",
    "### Number of Samples vs. Power\n",
    ">Indicate whether you will use the Bonferroni correction during your analysis phase, and give the number of pageviews you will need to power you experiment appropriately. \n",
    "\n",
    "I didn't use Bonferroni correction.\n",
    "[online calculator](http://www.evanmiller.org/ab-testing/sample-size.html) was used to calculate the sample sizes:\n",
    "* gross_conversion: \n",
    "Baseline conversion rate = 0.2063,\n",
    "Minimum Detectable Effect = 0.01,\n",
    "alpha = 0.05,\n",
    "1-beta = 0.8,\n",
    "N = 25835 \n",
    "\n",
    "* Retention\n",
    "Baseline conversion rate = 0.53\n",
    "Minimum Detectable Effect = 0.01,\n",
    "alpha = 0.05,\n",
    "1-beta = 0.8,\n",
    "N = 39087\n",
    "\n",
    "* net_conversion:\n",
    "Baseline conversion rate = 0.1093,\n",
    "Minimum Detectable Effect = 0.0075,\n",
    "alpha = 0.05,\n",
    "1-beta = 0.8,\n",
    "N = 27413\n",
    "\n",
    "** N is the sample sizes required in experiment and control groups. To calculate the acutal number of pageviews required, we also need to consider the clicking rate (0.08). For retention, as the samples coming from enrolled students, we also need to consider the probability of enrolling, given click (0.20625).  **\n",
    "** If we include Retention in the evaluation metrics, we need 39087$\\times$2/0.08/0.20625 = 4,737,819 pageviews. This will take about 4 months to finish the experiment, which is unrealistic long for the experiment.\n",
    "Use gross_conversion and net_conversion as evaluation metrics, we need to collect 27413$\\times$2/0.08 = 685,325 pageviews, which takes about 685,325/40,000 = 17.13 days if I use 100% traffic. Therefore, in the end I only use \"gross conversion\" and \"net conversion\" as evaluation metrics.  **\n",
    "\n",
    "### Duration vs. Exposure\n",
    ">Indicate what fraction of traffic you would divert to this experiment and, given this, how many days you would need to run the experiment. \n",
    "Give your reasoning for the fraction you chose to divert. How risky do you think this experiment would be for Udacity?\n",
    "\n",
    "** If I use 100% traffic for the experiment, I will need 685325/40000 = 17.13 days to run the experiment. The experiement is not risky because it won't  have any effect for students already enrolled and won't do harm to students want to take the courses, as it only adds one step/window to kindly remind students the efforts required for the courses before their enrollment. We are not dealing with sensitive data in the experiment and the experiment won't increase risks undertaken by participants. As long as we are certain that there is no other experiment needed to be run in the coming 18 days, we can use the highest traffic possible to finish the experiment in 18 days. This will need 685325/18/40000 = 95.18% traffic. **\n",
    "\n",
    "## Experiment Analysis\n",
    "### Sanity Checks\n",
    ">For each of your invariant metrics, give the 95% confidence interval for the value you expect to observe, the actual observed value, and whether the metric passes your sanity check. \n",
    "\n",
    "##### Number of cookies:\n",
    "* $N_{cont}$: 345543\n",
    "* $N_{exp}$: 344660\n",
    "* $N_{total}$: $N_{cont}$ + $N_{exp}$\n",
    "* expected probability in control and experiment group: P = 0.5\n",
    "* SE = $\\sqrt{\\frac{P\\times(1-P)}{N_{cont}+N_{exp}}}$ = 0.0006018\n",
    "* margin of error (m) = 1.96 * SE = 0.00118\n",
    "* CI = [0.5-m, 0.5+m] = [0.4988, 0.5012]\n",
    "* observed probability = $\\frac{N_{cont}}{N_{total}}$ = 0.5006, within the range of CI\n",
    "\n",
    "##### Number of clicks:\n",
    "* $N_{cont}$: 28378\n",
    "* $N_{exp}$: 28325\n",
    "* $N_{total}$: $N_{cont}$ + $N_{exp}$\n",
    "* expected probability in control and experiment group: P = 0.5\n",
    "* SE = $\\sqrt{\\frac{P\\times(1-P)}{N_{cont}+N_{exp}}}$ = 0.0021\n",
    "* margin of error (m) = 1.96 * SE = 0.0041\n",
    "* CI = [0.5-m, 0.5+m] = [0.4959, 0.5041]\n",
    "* observed probability = $\\frac{N_{cont}}{N_{total}}$ = 0.5005, within the range of CI\n",
    "\n",
    "##### Click through probability:\n",
    "* $N_{cont}$: 345543\n",
    "* control: P = 0.082125814\n",
    "* SE = $\\sqrt{\\frac{P\\times(1-P)}{N_{cont}}}$ = 0.000467\n",
    "* margin of error (m) = 1.96 * SE =0.000915\n",
    "* CI = [P-m, P+m] = [0.0812, 0.0830]\n",
    "* experiment: 0.0822\n",
    "\n",
    "**All observed values are within the confidence interval so all of them passed the sanity check.**\n",
    "\n",
    "\n",
    "## Result Analysis\n",
    "### Effect Size Tests\n",
    ">For each of your evaluation metrics, give a 95% confidence interval around the difference between the experiment and control groups. Indicate whether each metric is statistically and practically significant. \n",
    "\n",
    "* $P_{pool} = \\frac{X_{cont}+X_{exp}}{N_{cont}+N_{exp}}$\n",
    "* $SE = \\sqrt{P(1-P)(\\frac{1}{N_{exp}}+\\frac{1}{N_{cont}})}$\n",
    "* $ d = \\frac{X_{exp}}{N_{exp}} - \\frac{X_{cont}}{N_{cont}}$ \n",
    "* $\\mathrm{CI} = [\\mathrm{d}-1.96\\times\\mathrm{SE}, \\mathrm{d}+1.96\\times\\mathrm{SE}]$ \n",
    "\n",
    "##### Gross conversion:\n",
    "* $d_{min}$ = 0.01\n",
    "* $X_{cont}$ = 3785\n",
    "* $N_{cont}$ = 17293\n",
    "* $X_{exp}$ = 3423\n",
    "* $N_{exp}$ = 17260\n",
    "* $P_{pool}$ = 0.2086\n",
    "* SE = 0.004371\n",
    "* margin of error = 0.008568\n",
    "* d = -0.02055\n",
    "* CI = [-0.0291 , -0.0120]\n",
    "* ** statistically significant: Because CI doesn't include 0. **\n",
    "* ** practically significant: Because the result shows a decrease more than the practical significance boundary, $d_{min}$ (i.e., the absolute value of CI is larger than $d_{min}$).** \n",
    "\n",
    "##### Net conversion:\n",
    "* $d_{min}$ = 0.0075\n",
    "* $X_{cont}$ = 2033\n",
    "* $N_{cont}$ = 17293\n",
    "* $X_{exp}$ = 1945\n",
    "* $N_{exp}$ = 17260\n",
    "* $P_{pool}$ = 0.11513\n",
    "* SE = 0.003434\n",
    "* margin of error = 0.00673\n",
    "* d = -0.00487\n",
    "* CI = [-0.0116 , 0.00186]\n",
    "* ** not statistically significant: Because CI include 0. **\n",
    "* ** not practically significant: The CI is mainly in the negative region, showing that the experiment mainly results in a decrease in the net conversion. The result should at least increase $d_{min}$ to reach our expectation of practically significant but it failed (the whole CI is below the practical significance boundary, $d_{min}$). **\n",
    "\n",
    "\n",
    "### Sign Tests\n",
    ">For each of your evaluation metrics, do a sign test using the day-by-day data, and report the p-value of the sign test and whether the result is statistically significant. \n",
    "\n",
    "Use [this website](http://graphpad.com/quickcalcs/binomial1.cfm) to calculate p-value for the sign test\n",
    "##### Gross conversion:\n",
    "* days experiment group show higher gross conversion than control group: 4\n",
    "* total days: 23\n",
    "* two-tailed p-value: 0.0026\n",
    "* ** statisitcally significant** as p-value < alpha(0.05)\n",
    "##### Net conversion:\n",
    "* days experiment group show higher net conversion than control group: 10\n",
    "* total days: 23\n",
    "* two-tailed p-value = 0.6776 \n",
    "* ** no statisitcal significance** as p-value > alpha(0.05)\n",
    "\n",
    "### Summary\n",
    ">State whether you used the Bonferroni correction, and explain why or why not. If there are any discrepancies between the effect size hypothesis tests and the sign tests, describe the discrepancy and why you think it arose.\n",
    "\n",
    "I didn't use the Bonferroni correction. \n",
    "\n",
    "The Bonferroni correction is designed to reduce type I error. \n",
    "If the criteria for launching an experiment is that at least one of many evaluation metrics (say, N metrics) matches our expectation, then the probability that we make an mistake in launching the experiment will be $(1-(1-\\alpha)^N)$. This probability will become very high if we have lots of evaluation metrics. Therefore, Bonferroni correction adjusts the $\\alpha$ to be $\\frac{\\alpha}{N}$, to decrease the probablility for making mistake due to type I error.\n",
    "\n",
    "In this case, I need to fulfill the criteria of both gross conversion and net conversion metrics in order to launch the experiment. I will make a wrong decision about launching the experiment if the result of gross conversion or net conversion is actually wrong but I failed to reject it. This will be the type II error so the Bonferroni correction won't help in this case\n",
    "\n",
    "### Recommendation\n",
    ">Make a recommendation and briefly describe your reasoning.\n",
    "\n",
    "I won't recommend to launch the experiment. \n",
    "\n",
    "First, the analysis of gross conversion metric is significantly (both statistically and practically) smaller in the experiment group. Therefore, we know the experiment successfully achieved the first goal: reduce the number of students left the free trial. Second, the analysis of net conversion metric shows no significant changes in the experiment group, suggesting the experiment didn't reduce the number of students already paid. However, the change of net conversion metric is not practically significant for us to launch the experiment. The much broader negative range of CI than its positive range also implies the high chance of decreasing the net conversion metric, which disagrees with our second goal of the experiment. \n",
    "\n",
    "\n",
    "\n",
    "## Follow-Up Experiment: How to Reduce Early Cancellations\n",
    ">If you wanted to reduce the number of frustrated students who cancel early in the course, what experiment would you try? Give a brief description of the change you would make, what your hypothesis would be about the effect of the change, what metrics you would want to measure, and what unit of diversion you would use. Include an explanation of each of your choices.\n",
    "\n",
    "I think experiences from students finished nanodegree will help to guide students to overcome the frustrations. If we can provide video/article from students sucessfully finished the nanodegree with different backgrounds telling about what kind of difficulties they encountered in their nanodegree study, how they deal with all challenges, how they plan their time to finish the nanodegree and how the skills learned from the course are used in their current lives... This implementation can help the current enrolled students to learn from students who finished the nanodegree successfully and having similar background to overcome the frustrations as well as get motivated.\n",
    "\n",
    "In the experiment, a new button \"see how other students get the nanodegree\" will be added to the main page of enrolled users. Only enrolled users can see the button. If they click the button then they can watch the video/article from students sucessfully finished the nanodegree with different backgrounds. \n",
    "\n",
    "The hypothesis is that the experiences of students who finished the nanodegree successfully and having similar background can help the frustrated students to deal with the challenges and frustrations, thus reducing the number of cancellation. The experiment should not significantly reduce the number of students that already paid for the nanodegree. \n",
    "\n",
    "The unit of diversion will be user-id, as every enrolled students have their own user-id and the button can only be seen by students with user-id. \n",
    "\n",
    "The invariant metrics will be \"num_view = the number of unique enrolled user-id view their main pages (the page where the button exists)\" and \"num_enroll = the number of users enrolled in the free trial\". We can use these metrics to verify if the samples are randomly assigned in the experiment and control groups.\n",
    "\n",
    "The evaluation metrics will be \"percent_paid = the number of user-ids past the 14-day boundary divided by the number of unique enrolled user-id view their main pages\" and \"percent_cancelled = the number of cancelled user-id divided by the number of unique enrolled user-id view their main pages\". The experiment will be launched only if the criteria for both percent_paid and percent_cancelled metrics are matched. The percent_paid in the experiment group should not significantlly lower than that in the control group, so we know the experiment won't have negative effect the students that already paid for the nanodegree. The percent_cancelled in the experiment group should be significanly lower than that in the control group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [DAND]",
   "language": "python",
   "name": "Python [DAND]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
